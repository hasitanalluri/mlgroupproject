{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIsOVhFB4r2J"
      },
      "source": [
        "### 1. Abstract\n",
        "From 2020 through 2024, the COVID-19 pandemic placed unprecedented strain on the United States' healthcare system, with hospitals facing surges in patient volume and widespread staffing shortages. This paper explores the feasibility of using publicly available hospital capacity data to predict staffing shortages at the state level. Using the U.S. Department of Health and Human Services' COVID-19 Reported Patient Impact and Hospital Capacity dataset, we evaluate whether critical staffing shortages can be predicted using supervised machine learning models.\n",
        "\n",
        "After performing data cleaning and preprocessing--including handling missing values and engineering relevant time-based features--we formulated the task as a binary classification problem: predicting whether a state reported a staffing shortage on a given day. We trained multiple models, including logistic regression for interpretability and tree-based ensemble methods for improved performance and robustness.\n",
        "\n",
        "Our models achieved meaningful predictive performance, with the tree-based models outperforming logistic regression, particularly in identifying true staffing shortage events with fewer false positives. Key predictive features included the number of currently hospitalized COVID-19 patients, ICU occupancy rates, and whether a staffing shortage had been recently reported in the same state. In particular, recent trends in hospital strain often served as early warning signs of upcoming shortages.\n",
        "\n",
        "These results suggest that it is indeed possible to make actionable, data-driven predictions about staffing shortages in near real time. Such models could be incorporated into decision support tools, enabling more proactive staffing and resource allocation strategies. Despite challenges such as data gaps and inconsistent reporting across states, our findings show that machine learning can effectively support public health planning and crisis response.\n",
        "\n",
        "Future work could explore incorporating additional real-time data sources, like local case rates or mobility data, and adapting the models to individual hospital systems for finer-grained predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2y93DYy4r2K"
      },
      "source": [
        "### 2. Introduction\n",
        "The COVID-19 pandemic placed unprecedented strain on hospitals across the United States, forcing medical systems to respond to rapidly shifting demands on staffing, patient volume, and resource availability. One of the most pressing issues faced by healthcare providers during this period was the widespread occurrence of hospital staffing shortages—days when facilities lacked sufficient personnel to provide adequate care. These shortages had far-reaching consequences, delaying treatments, limiting ICU capacity, and exacerbating burnout among medical workers.\n",
        "\n",
        "In this study, we explore whether these critical staffing shortages can be predicted in advance using data collected and published by the U.S. Department of Health and Human Services. The dataset, which includes daily, state-level hospital metrics from January 2020 through April 2024, captures detailed information on hospital capacity, COVID-19 burden, staffing shortages, and related metrics. Our central research question is whether supervised machine learning models can reliably predict whether a state will report a staffing shortage on a given day based on these inputs.\n",
        "\n",
        "To investigate this, we cleaned and prepared a large dataset with over 130 features and more than 70,000 observations. After addressing missingness and scaling numerical values, we transformed the outcome variable into a binary label representing whether more than 20% of hospitals in a state reported a staffing shortage—a threshold representing \"high strain.\" We trained several predictive models, starting with logistic regression for interpretability and extending to more complex models such as random forests for improved performance and insight into feature importance.\n",
        "\n",
        "The logistic regression model achieved a test accuracy of nearly 89%, with a low false positive rate (5%) and a reasonably high true positive rate (70%), indicating that our approach was both accurate and conservative in identifying actual strain events. The results suggest that we can indeed anticipate hospital staffing shortages using state-level data, which has promising implications for proactive staffing and resource planning.\n",
        "\n",
        "To further understand what drives staffing strain, we used a random forest model to evaluate feature importance. The most predictive variable was whether hospitals anticipated a staffing shortage in the coming week—a subjective indicator that turned out to be more informative than several objective measures. Other important features included inpatient bed usage, COVID-19 death coverage, and state-level identifiers. These results highlight the value of combining both quantitative indicators and qualitative signals when forecasting hospital stress.\n",
        "\n",
        "The remainder of this paper outlines our data preparation methods, modeling strategies, and evaluation metrics. We conclude by discussing the practical implications of our findings and how similar predictive tools might help public health systems better prepare for staffing challenges in future health crises.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvrQCc2r4r2K"
      },
      "source": [
        "### 3. Data\n",
        "We obtained our dataset from the COVID-19 Hospital Reported Patient Impact and Hospital Capacity database, which includes state-level hospital data on COVID-19 cases, staffing shortages, and hospital capacity. The dataset spans from January 2020 until April 2024, and contains 135 variables, each row representing a daily report for a specific state or territory in the United States.\n",
        "\n",
        "The dataset includes several critical variables, such as the number of hospitals reporting critical staffing shortages, the number of COVID-19 hospital admissions, and the utilization of inpatient beds (both overall and specifically for COVID-19 patients). We also detemrined variables related to staffing availability and pediatric care, among others. Our dataset covers all 50 U.S. states as well as territories: Puerto Rico (PR), U.S. Virgin Islands (VI), American Samoa (AS), and the District of Columbia (DC). The \"state\" variable identifies the state or territory, while the \"date\" variable marks the daily data collection. The date range spans from January 1, 2020, to April 27, 2024.\n",
        "\n",
        "We began by converting the \"date\" column to a datetime format and creating a timestamp in seconds since the Unix epoch for time-based analysis. We also explored key variables, such as the number of hospitals reporting critical staffing shortages. After noticing discrepancies in the scales of the variables, we used the inverse hyperbolic sine (arcsinh) transformation to standardize the values for easier comparison. To handle missing data, we identified and dropped rows with critical missing values, such as inpatient bed usage or hospital onset COVID-19 data, while acknowledging the significant gaps in variables related to pediatric care and certain therapeutic supplies. We also flagged categorical variables, such as \"state,\" for potential additional cleaning.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29SJTWsY4r2K"
      },
      "source": [
        "### 4. Methods\n",
        "We plan to perform supervised learning with regression models to predict hospital strain and COVID-19 onset under two scenarios:\n",
        "\n",
        "1. Predicting the percentage of hospitals with critical staff shortages based on the number of inpatient COVID-19 patients and the number of inpatient beds used.\n",
        "\n",
        "2. Predicting the onset of COVID-19 in hospitals based on inpatient bed utilization and critical staff shortages.\n",
        "\n",
        "We will begin with linear regression to model the relationship between COVID-19 hospitalizations and hospital bed utilization. This will allow us to interpret how increases in COVID-19 cases and staffing shortages are associated with higher usage of hospital resources. Linear regression will also provide us with interpretable coefficients and diagnostic tools, such as residual analysis and R², to evaluate the model's fit.\n",
        "\n",
        "For classification tasks, we will use logistic regression to categorize state-day observations as “high strain” or “low strain” based on thresholds for hospital utilization or staff shortages. This approach will provide interpretable odds ratios and is appropriate for binary outcomes.\n",
        "\n",
        "To capture non-linear patterns, interaction effects, and threshold behavior (e.g., a certain level of COVID-19 admissions triggering capacity overload), we will use decision trees. Building on decision trees, we will apply random forests for improved generalization and stability. Feature importance scores from the random forest model will help identify the drivers of hospital strain.\n",
        "\n",
        "We may also use Principal Component Analysis (PCA) to reduce dimensionality, as many of our numerical features are correlated (e.g., different types of inpatient bed usage). PCA will simplify the feature space while preserving most of the variance.\n",
        "\n",
        "All models will be evaluated using cross-validation to ensure generalizability and avoid overfitting. We will compare models based on R², RMSE, and MSE for regression tasks, and accuracy, precision, recall, and F1 score for classification tasks.\n",
        "\n",
        "We will evaluate our models using R², RMSE, and MSE for regression, and using accuracy, precision, recall, and F1 score for classification.\n",
        "\n",
        "Several challenges are anticipated during the analysis:\n",
        "1. Data Quality: Incomplete, inconsistent, or erroneous data may affect model performance. In particular, missing data may be an issue if certain states do not report consistently. We will use imputation techniques (e.g., mean, median, or regression-based imputation) and may exclude unreliable data points.\n",
        "\n",
        "2. Overfitting: The large dataset and numerous variables increase the risk of overfitting. We will address this by using cross-validation and regularization, and by splitting the data into training and test sets. PCA may also be used to reduce the number of features.\n",
        "\n",
        "3. Non-Stationarity: Hospital rates may fluctuate independently of COVID-19 cases, and trends or seasonality could affect the data. To address this, we will use coverage metrics to adjust for population size and calculate the number of patients per hospital. We may also apply differencing or detrending to make the data stationary.\n",
        "\n",
        "We will present our results through tables and visualizations:\n",
        "- Regression Models: Evaluated using R², RMSE, and residual plots.\n",
        "- Classification Results: Presented using confusion matrices and F1 scores.\n",
        "- Visualizations: Bar charts for feature importances, line plots for hospital utilization trends, scatter plots, and heatmaps for correlations. If time permits, we may explore how model predictions vary by region or change over time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnIXeACJ4r2L"
      },
      "source": [
        "### 5. Results\n",
        "We began our analysis by constructing the outcome variable, critical_staffing_shortage_today_prop, defined as the proportion of hospitals in a state reporting a critical staffing shortage on a given day. To focus our analysis on meaningful strain events, we binarized this variable into a new column, strain, where a value of 1 indicated that more than 20% of hospitals in a state reported staffing shortages on a given day. This threshold was chosen based on the 75th percentile of the distribution, which corresponded to a 20% shortage rate, as shown in the boxplot and descriptive statistics (Figure 1). Approximately 25% of observations met this high-strain condition.\n",
        "\n",
        "After cleaning the dataset by removing rows with missing values in key variables and normalizing numerical features, we trained a logistic regression model to classify observations as high-strain or low-strain. To account for state-level fixed effects, we included one-hot encoded state indicators as predictors. The model was trained on 80% of the data and tested on the remaining 20%.\n",
        "\n",
        "The logistic regression model achieved strong predictive performance, with an accuracy of 89.2% on the training set and 88.9% on the testing set. The confusion matrix indicated high precision and recall for both classes (Figure 2). Specifically, the model achieved:\n",
        "\n",
        "True Positive Rate (Recall for High Strain): 0.70\n",
        "\n",
        "True Negative Rate (Recall for Low Strain): 0.95\n",
        "\n",
        "False Positive Rate: 0.05\n",
        "\n",
        "False Negative Rate: 0.30\n",
        "\n",
        "Overall Accuracy: 0.89\n",
        "\n",
        "These results suggest that the model was highly effective at identifying low-strain days while maintaining reasonable sensitivity to high-strain conditions. The low false positive rate is particularly beneficial for real-world deployment, as it minimizes the risk of over-allocating resources. However, the false negative rate of 30% indicates that some high-strain days may go undetected, potentially delaying critical interventions. This limitation is likely due to class imbalance, as the majority of state-day observations represented low-strain conditions.\n",
        "\n",
        "To further investigate which features were most predictive of staffing strain, we trained a Random Forest Regressor with a maximum tree depth of 10. The model was used to predict the continuous proportion of hospitals reporting staffing shortages, and the top 10 features were identified based on their mean decrease in impurity (Figure 3).\n",
        "\n",
        "The most important feature was whether hospitals in a state anticipated a staffing shortage within the next week (critical_staffing_shortage_anticipated_within_week_yes), followed by the inverse of that same feature (_no). This finding reinforces the idea that hospitals’ internal assessments of upcoming challenges are strong indicators of actual strain. Other important predictors included:\n",
        "\n",
        "inpatient_beds_used_coverage: The proportion of inpatient beds in use\n",
        "\n",
        "SC: A state-level indicator for South Carolina, suggesting unique reporting patterns or strain levels\n",
        "\n",
        "deaths_covid_coverage: A metric representing hospital-reported COVID-19 deaths\n",
        "\n",
        "critical_staffing_shortage_today_not_reported: A possible proxy for missing or delayed reporting\n",
        "\n",
        "inpatient_beds_utilization_denominator: Total beds considered in utilization calculations\n",
        "\n",
        "previous_day_deaths_covid_and_influenza_coverage: Lagged mortality data\n",
        "\n",
        "total_patients_hospitalized_confirmed_influenza_and_covid_coverage: Combined patient counts\n",
        "\n",
        "inpatient_beds: Raw bed counts\n",
        "\n",
        "The inclusion of both objective metrics (e.g., bed usage, death counts) and more qualitative or proxy measures (e.g., anticipated shortages, state identifiers) among the top features suggests that both data types are valuable in predicting staffing strain. Notably, subjective forecasting by hospital administrators (“anticipated shortage”) proved more predictive than current conditions alone, highlighting the potential of integrating anticipatory indicators into predictive models.\n",
        "\n",
        "Finally, the Random Forest model achieved an R² of approximately 0.70 on the test data, indicating strong explanatory power for a regression-based view of strain intensity.\n",
        "\n",
        "Together, these results affirm that machine learning models trained on publicly available hospital data can accurately predict staffing shortages, and that a combination of operational metrics and human judgment features are critical for high performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vh50hzcl4r2L"
      },
      "source": [
        "### 6. Conclusion\n",
        "The COVID-19 pandemic exposed systemic vulnerabilities in the United States healthcare system, one of the most critical being staffing shortages in hospitals. As frontline workers navigated overwhelming caseloads, understaffed facilities, and personal risk, the ability to anticipate resource constraints became a pressing public health priority. In this study, we set out to evaluate whether it is possible to predict hospital staffing shortages at the state level using publicly available COVID-19 hospital data, and to identify which features are most predictive of these shortages.\n",
        "\n",
        "We formulated this task as a supervised machine learning problem, using classification models to identify whether a state was experiencing a high level of hospital staffing strain on a given day (defined as >20% of hospitals reporting shortages). After a thorough data cleaning and preprocessing process, we trained and evaluated both interpretable and non-linear models. Logistic regression was used for its transparency and baseline performance, while a Random Forest Regressor provided insights into the drivers of strain and allowed for richer feature analysis.\n",
        "\n",
        "Our findings were encouraging. The logistic regression model achieved strong predictive performance, with an accuracy of nearly 89% on unseen test data and a low false positive rate of 5%. This means the model is unlikely to overreact and trigger unnecessary interventions in low-strain contexts—an important consideration in real-world deployment. At the same time, the model correctly identified 70% of high-strain cases, suggesting that a majority of actual shortages could be predicted in advance.\n",
        "\n",
        "Feature Insights and Key Predictors\n",
        "Perhaps the most illuminating finding was the predictive power of hospitals’ own anticipations of strain. The feature critical_staffing_shortage_anticipated_within_week_yes was by far the most important in the random forest model. This suggests that administrators’ on-the-ground insights about staffing trends and operational conditions are not just useful—they are among the most reliable indicators available. In a context where lagging indicators (e.g., COVID deaths, hospitalization rates) can miss early warning signs, these subjective forecasts offer timely and actionable signals.\n",
        "\n",
        "Other top features included inpatient_beds_used_coverage, COVID-19 death rates, and state-level identifiers. Interestingly, some of the most predictive features were not directly about staffing but instead described capacity usage and mortality trends. This implies that staffing shortages are often downstream effects of broader systemic strain and that predictive models benefit from a holistic view of hospital operations.\n",
        "\n",
        "Complications and Challenges\n",
        "While the results demonstrate clear potential for predictive modeling of hospital staffing strain, several challenges emerged throughout the project. Rather than treating these issues as limitations, we present them here as areas of future exploration.\n",
        "\n",
        "1. Class Imbalance and Model Sensitivity:\n",
        "Roughly 75% of our observations fell into the “low strain” category. This imbalance skewed the model’s predictions toward the majority class, contributing to a higher false negative rate (30%). Though still within acceptable bounds, this means a non-trivial number of actual high-strain days were not flagged. Techniques such as SMOTE (Synthetic Minority Oversampling Technique), class weighting, or threshold tuning could be explored to improve sensitivity to high-strain cases.\n",
        "\n",
        "2. Data Quality and Missingness:\n",
        "Our dataset was large and rich, but not without its flaws. Many columns had inconsistent or sparse reporting across states and dates, especially in categories related to pediatric care and specialized therapeutics. While we used conservative missing-data handling methods (e.g., dropping rows with null values for key features), future work could apply more sophisticated imputation techniques or model-based approaches that can account for uncertainty in missingness. Additionally, collaboration with public health agencies to improve data completeness and consistency would strengthen the foundation of any predictive system.\n",
        "\n",
        "3. Subjective vs. Objective Indicators:\n",
        "One of the most compelling findings was that subjective indicators (e.g., anticipated staffing shortages) were more predictive than objective metrics like bed counts or case numbers. This raises important methodological and philosophical questions: Should predictive models rely heavily on self-reported expectations? How can we validate or calibrate such data to ensure reliability? Further work might explore incorporating confidence intervals or cross-checking forecasts against outcomes over longer timeframes.\n",
        "\n",
        "4. Temporal Dynamics and Drift:\n",
        "Although we used a static train-test split for simplicity, real-world deployment would require consideration of temporal trends and concept drift. For instance, what if the relationship between bed usage and staffing strain changes over time due to policy interventions, new variants, or seasonal effects? Future studies could use rolling-window validation or time-series cross-validation to better simulate prospective prediction.\n",
        "\n",
        "5. Granularity of Predictions:\n",
        "This study focused on state-level predictions, which offer actionable insights for federal or state agencies. However, hospital systems operate at more localized levels. In future work, similar models could be developed at the county or individual hospital level, assuming appropriate data access and privacy safeguards. This would enhance responsiveness and allow more precise allocation of staff or resources.\n",
        "\n",
        "6. Expanding Predictive Inputs:\n",
        "While we focused on COVID-19 hospital data, other sources could improve prediction. Mobility data, local case forecasts, vaccination rates, and even social media sentiment could serve as leading indicators of emerging healthcare pressure. Integrating multiple data modalities might yield earlier or more accurate predictions, especially in future public health crises that differ from COVID-19 in key respects.\n",
        "\n",
        "Toward Real-Time Decision Support\n",
        "Ultimately, the strength of our models lies not only in their predictive accuracy but also in their interpretability and potential utility for real-time decision-making. The fact that high-stakes events like staffing shortages can be anticipated using existing public datasets is a hopeful sign. With further refinement, such models could serve as the foundation for operational dashboards that alert health administrators when and where to mobilize personnel, preemptively divert patients, or postpone elective procedures.\n",
        "\n",
        "As the public health landscape continues to evolve, the ability to translate noisy, decentralized data into clear, actionable insight will remain a vital capability. By highlighting both the promise and the pitfalls of predictive modeling in this context, we hope this work contributes to the development of more resilient, responsive healthcare systems in the years ahead."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bxwtt6PW4r2L"
      },
      "source": [
        "### 7. References/Bibliography\n",
        "\n",
        "U.S. Department of Health & Human Services. (2025). COVID-19 Reported Patient Impact and Hospital Capacity by State Timeseries (RAW). HealthData.gov. Retrieved from https://healthdata.gov/dataset/COVID-19-Reported-Patient-Impact-and-Hospital-Capa/6xf2-c3ie\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
